{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2e2aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b61f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "\n",
    "    activation_fn: callable\n",
    "\n",
    "    def __init__(self, arq, activation_fn):\n",
    "\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "\n",
    "        self.activation_fn = activation_fn\n",
    "\n",
    "        self.neural_network = nn.ModuleList()\n",
    "        \n",
    "        for i in range(len(arq)-1):\n",
    "            self.neural_network.append(nn.Linear(arq[i], arq[i+1]))\n",
    "            nn.init.xavier_normal_(self.neural_network[i].weight)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        for layer in self.neural_network[:-1]: x = self.activation_fn(layer(x))\n",
    "        x = self.neural_network[-1](x)  # Sem ativação na última camada\n",
    "        return x\n",
    "    \n",
    "    def save(self, local: str):\n",
    "\n",
    "        torch.save(self.state_dict(), local)\n",
    "\n",
    "    def load(self, local: str):\n",
    "\n",
    "        self.load_state_dict(torch.load(local))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f341f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inversive_PINN:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.param = nn.Parameter(torch.tensor(1., requires_grad = True))\n",
    "\n",
    "    def residual_loss(self, model, x):\n",
    "\n",
    "        # Passagem da entrada pela rede\n",
    "        x.requires_grad_(True)\n",
    "        u = model(x)\n",
    "\n",
    "        # Calculo dos gradientes\n",
    "        du = torch.autograd.grad(u, x, grad_outputs=torch.ones_like(u), create_graph=True)[0]\n",
    "        d2u = torch.autograd.grad(du, x, grad_outputs=torch.ones_like(du), create_graph=True)[0]\n",
    "        \n",
    "        # Resíduo da equação diferencial\n",
    "        residual = d2u + self.param * u\n",
    "        \n",
    "        return torch.mean(residual**2)\n",
    "    \n",
    "    def data_loss(self, model, input, target):\n",
    "\n",
    "        u_pred = model(input)\n",
    "        return torch.mean((u_pred - target)**2)\n",
    "    \n",
    "    def trainning(self, model, x_data, u_data, x_residual, epochs = 1_000, learning_rate = 1e-3, step = 100):\n",
    "\n",
    "        optmizer = torch.optim.Adam([\n",
    "            {'params': model.parameters()},\n",
    "            {'params': [self.param]}\n",
    "            ],\n",
    "            lr = learning_rate\n",
    "        )\n",
    "        loss_history = []\n",
    "\n",
    "        for ep in range(epochs):\n",
    "\n",
    "            optmizer.zero_grad()\n",
    "\n",
    "            loss_data = self.data_loss(model, x_data, u_data)\n",
    "            loss_residual = self.residual_loss(model, x_residual)\n",
    "\n",
    "            loss = loss_data + loss_residual\n",
    "            loss.backward()\n",
    "\n",
    "            optmizer.step()\n",
    "\n",
    "            if ep % step == 0 : print(f'Epoch {ep}, Loss: {loss.item()}, Lambda: {self.param.item()}')\n",
    "            loss_history.append(loss.item())\n",
    "\n",
    "        return loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a9b440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(lambda_true=5.0, n_data=100, noise_level=0.05):\n",
    "\n",
    "    x_data = torch.linspace(0, 1, n_data).view(-1, 1)\n",
    "    u_data = torch.sin(np.sqrt(lambda_true) * x_data)\n",
    "    u_data += noise_level * torch.randn_like(u_data)\n",
    "    \n",
    "    return x_data, u_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e0e8cc85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500, 1]), torch.Size([500, 1]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "    [1] + 3 * [100] + [1],\n",
    "    nn.Tanh()\n",
    ")\n",
    "inverse_pinn = Inversive_PINN()\n",
    "\n",
    "x_data, u_data = generate_data(lambda_true = 4.0, n_data = 500)\n",
    "x_residual = torch.linspace(0, 1, 100).view(-1, 1).requires_grad_(True)\n",
    "\n",
    "x_data.shape, u_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61a88108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.169370412826538, Lambda: 0.9990000128746033\n",
      "Epoch 1000, Loss: 0.010887625627219677, Lambda: 0.996890664100647\n",
      "Epoch 2000, Loss: 0.010426095686852932, Lambda: 1.0880730152130127\n",
      "Epoch 3000, Loss: 0.009766711853444576, Lambda: 1.2216764688491821\n",
      "Epoch 4000, Loss: 0.009012575261294842, Lambda: 1.3834940195083618\n",
      "Epoch 5000, Loss: 0.008179369382560253, Lambda: 1.5646663904190063\n",
      "Epoch 6000, Loss: 0.007425185292959213, Lambda: 1.7422140836715698\n",
      "Epoch 7000, Loss: 0.0067487978376448154, Lambda: 1.9126628637313843\n",
      "Epoch 8000, Loss: 0.00622345507144928, Lambda: 2.07961368560791\n",
      "Epoch 9000, Loss: 0.005586522631347179, Lambda: 2.2393221855163574\n",
      "Epoch 10000, Loss: 0.005135396495461464, Lambda: 2.383657217025757\n",
      "Epoch 11000, Loss: 0.0047171940095722675, Lambda: 2.5164287090301514\n",
      "Epoch 12000, Loss: 0.004397244658321142, Lambda: 2.6447978019714355\n",
      "Epoch 13000, Loss: 0.004200169816613197, Lambda: 2.76465106010437\n",
      "Epoch 14000, Loss: 0.003813691670075059, Lambda: 2.869314670562744\n",
      "Epoch 15000, Loss: 0.003733239369466901, Lambda: 2.9700915813446045\n",
      "Epoch 16000, Loss: 0.0034705789294093847, Lambda: 3.0643081665039062\n",
      "Epoch 17000, Loss: 0.003847897984087467, Lambda: 3.151055097579956\n",
      "Epoch 18000, Loss: 0.0031259935349226, Lambda: 3.225905656814575\n",
      "Epoch 19000, Loss: 0.004316748585551977, Lambda: 3.3000595569610596\n",
      "Epoch 20000, Loss: 0.003237338736653328, Lambda: 3.367957353591919\n",
      "Epoch 21000, Loss: 0.002884198911488056, Lambda: 3.3943042755126953\n",
      "Epoch 22000, Loss: 0.002857389161363244, Lambda: 3.436641216278076\n",
      "Epoch 23000, Loss: 0.0027714092284440994, Lambda: 3.4904606342315674\n",
      "Epoch 24000, Loss: 0.002804363612085581, Lambda: 3.5417306423187256\n",
      "Epoch 25000, Loss: 0.0031313563231378794, Lambda: 3.589185953140259\n",
      "Epoch 26000, Loss: 0.0026477521751075983, Lambda: 3.631720781326294\n",
      "Epoch 27000, Loss: 0.0026270223315805197, Lambda: 3.6460466384887695\n",
      "Epoch 28000, Loss: 0.0026066384743899107, Lambda: 3.672621250152588\n",
      "Epoch 29000, Loss: 0.002584570785984397, Lambda: 3.70524525642395\n",
      "Epoch 30000, Loss: 0.0026186739560216665, Lambda: 3.736146926879883\n",
      "Epoch 31000, Loss: 0.0025655061472207308, Lambda: 3.764200448989868\n",
      "Epoch 32000, Loss: 0.002701559802517295, Lambda: 3.789595365524292\n",
      "Epoch 33000, Loss: 0.00253750360570848, Lambda: 3.7935280799865723\n",
      "Epoch 34000, Loss: 0.002528858371078968, Lambda: 3.8100035190582275\n",
      "Epoch 35000, Loss: 0.0028769157361239195, Lambda: 3.8304009437561035\n",
      "Epoch 36000, Loss: 0.0025135662872344255, Lambda: 3.849238157272339\n",
      "Epoch 37000, Loss: 0.0025404426269233227, Lambda: 3.8657419681549072\n",
      "Epoch 38000, Loss: 0.0025083201471716166, Lambda: 3.8622851371765137\n",
      "Epoch 39000, Loss: 0.0025034418795257807, Lambda: 3.872711420059204\n",
      "Epoch 40000, Loss: 0.002499331720173359, Lambda: 3.8865833282470703\n",
      "Epoch 41000, Loss: 0.0024969473015516996, Lambda: 3.8990776538848877\n",
      "Epoch 42000, Loss: 0.002495403168722987, Lambda: 3.9104700088500977\n",
      "Epoch 43000, Loss: 0.0025215207133442163, Lambda: 3.9208362102508545\n",
      "Epoch 44000, Loss: 0.0025043953210115433, Lambda: 3.9299097061157227\n",
      "Epoch 45000, Loss: 0.0024874950759112835, Lambda: 3.930279493331909\n",
      "Epoch 46000, Loss: 0.0025684048887342215, Lambda: 3.938931941986084\n",
      "Epoch 47000, Loss: 0.002512396313250065, Lambda: 3.946367025375366\n",
      "Epoch 48000, Loss: 0.0029698635917156935, Lambda: 3.9529929161071777\n",
      "Epoch 49000, Loss: 0.0024831907358020544, Lambda: 3.9589943885803223\n"
     ]
    }
   ],
   "source": [
    "loss_history = inverse_pinn.trainning(model, x_data, u_data, x_residual, epochs = 50_000, step = 1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08407efc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
